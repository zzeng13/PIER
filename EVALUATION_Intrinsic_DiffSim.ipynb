{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c535c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we compute the intrinsic evaluation metric:  \n",
    "# - mean inter-type cosine similarity ((DiffSim â†“) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562512ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from src.utils.data_util import DataHandlerCLS\n",
    "from src.train_valid_test_step import *\n",
    "from config import Config as config\n",
    "from torch.multiprocessing import set_start_method\n",
    "from src.model.bart_adapters import BartAdapterCombined, BartAdapter\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy import spatial\n",
    "import json\n",
    "from transformers import BartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb4e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def write_json_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cd019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    # token_embeddings: [batch_size, max_seq_len, hidden_dim]\n",
    "    # token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b804130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of idioms: 1521\n",
      "Training dataset size: 32693\n",
      "Validation dataset size: 4102\n",
      "Testing dataset size: 4080\n",
      "Load base model from facebook/bart-base\n",
      "=> Initializing Adapters with Fusion Module...\n",
      "\n",
      "[COMPOSITIONAL MODULE]: \n",
      "==> Using BART output as compositional embedding!\n",
      "\n",
      "[NON-COMPOSITIONAL MODULE]: \n",
      "==> Loading Adapter from /home/zzeng/workspace/UIUC_research/RepresentationLearning/models/PIER/checkpoints/bart-adapters_non-compositional_magpie_random-GIEA/best/\n",
      "==> Non-compositional adapter loaded!\n",
      "\n",
      "[COMBINED MODULE]: \n",
      "==> Adding fusion module!\n",
      "Fuse[compositional, non-compositional]\n",
      "==> Loading Adapter Fusion from /home/zzeng/workspace/UIUC_research/RepresentationLearning/models/PIER/checkpoints/bart-adapters_fusion_magpie_random-PIER/best/\n",
      "==>  Adapter Fusion Loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartAdapterCombined(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (invertible_adapters): ModuleDict()\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (1): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (2): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (3): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (4): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (5): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (cross_attention_adapters): BartCrossAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (1): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (cross_attention_adapters): BartCrossAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (2): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (cross_attention_adapters): BartCrossAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (3): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (cross_attention_adapters): BartCrossAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (4): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (cross_attention_adapters): BartCrossAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (5): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention_adapters): BartSelfAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (output_adapters): BartOutputAdaptersModule(\n",
       "              (adapters): ModuleDict(\n",
       "                (compositional): Adapter()\n",
       "                (non-compositional): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (compositional,non-compositional): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "            (cross_attention_adapters): BartCrossAttentionAdaptersModule(\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "              (adapter_flow_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (invertible_adapters): ModuleDict()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (cosine_loss): CosineEmbeddingLoss()\n",
       "  (nll_loss): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "data_handler = DataHandlerCLS()\n",
    "\n",
    "# Manage and initialize model\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Initialize model\n",
    "model = BartAdapterCombined(data_handler.config)\n",
    "model.to(config.DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e234af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter Name: fusion\n",
      "Adapter Split: random\n",
      "Task Split: random\n"
     ]
    }
   ],
   "source": [
    "# print out current test model information\n",
    "print('Adapter Name: {}'.format(config.ADAPTER_NAME))\n",
    "print('Adapter Split: {}'.format(config.SPLIT))\n",
    "print('Task Split: {}'.format(config.CLS_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9d98ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "# Run prediction on test set\n",
    "idioms2embed = {}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "bbar = tqdm(enumerate(data_handler.testset_generator),\n",
    "                ncols=100, leave=False, total=data_handler.config.num_batch_test)\n",
    "\n",
    "for idx, data in bbar:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # model forward pass to compute loss\n",
    "        outputs = model.model(**data['inputs'])\n",
    "    embeds = model.mean_pooling(outputs.last_hidden_state, data['phrase_masks'])\n",
    "    embeds = embeds.detach().cpu().numpy()\n",
    "    idioms = data['idioms']\n",
    "    labels = data['labels']\n",
    "    for i, idiom in enumerate(idioms): \n",
    "        if idiom not in idioms2embed: \n",
    "            idioms2embed[idiom] = {'idiomatic':[], 'literal':[]}\n",
    "        if labels[i] == 1: \n",
    "            idioms2embed[idiom]['idiomatic'].append(embeds[i])\n",
    "            assert 'literal' in idioms2embed[idiom]\n",
    "        else: \n",
    "            idioms2embed[idiom]['literal'].append(embeds[i])\n",
    "            assert 'idiomatic' in idioms2embed[idiom]\n",
    "    \n",
    "            \n",
    "            \n",
    "bbar = tqdm(enumerate(data_handler.validset_generator),\n",
    "                ncols=100, leave=False, total=data_handler.config.num_batch_valid)\n",
    "\n",
    "for idx, data in bbar:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # model forward pass to compute loss\n",
    "        outputs = model.model(**data['inputs'])\n",
    "    embeds = model.mean_pooling(outputs.last_hidden_state, data['phrase_masks'])\n",
    "    embeds = embeds.detach().cpu().numpy()\n",
    "    idioms = data['idioms']\n",
    "    labels = data['labels']\n",
    "    for i, idiom in enumerate(idioms): \n",
    "        if idiom not in idioms2embed: \n",
    "            idioms2embed[idiom] = {'idiomatic':[], 'literal':[]}\n",
    "        if labels[i] == 1: \n",
    "            idioms2embed[idiom]['idiomatic'].append(embeds[i])\n",
    "        else: \n",
    "            idioms2embed[idiom]['literal'].append(embeds[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb0464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean inter-type cosine similarity (DiffSim):\n",
      "0.32302344495937846\n"
     ]
    }
   ],
   "source": [
    "# note that in here, we keep the idioms with larger than one literal sentences and one idiomatic sentences in the test set\n",
    "idioms2embed_keep = {}\n",
    "count = 0 \n",
    "for idiom in idioms2embed.keys(): \n",
    "    if len(idioms2embed[idiom]['idiomatic']) > 0 and len(idioms2embed[idiom]['literal']) > 0: \n",
    "        idioms2embed_keep[idiom] = {'idiomatic': [], 'literal': []}\n",
    "        count += len(idioms2embed[idiom]['idiomatic'])\n",
    "        count += len(idioms2embed[idiom]['literal'])\n",
    "        idioms2embed_keep[idiom]['idiomatic'] =  np.mean(idioms2embed[idiom]['idiomatic'], 0).tolist()\n",
    "        idioms2embed_keep[idiom]['literal'] =  np.mean(idioms2embed[idiom]['literal'], 0).tolist()\n",
    "\n",
    "idioms_embed_cossim = []\n",
    "idiom_to_cosim = {}\n",
    "for idiom in idioms2embed_keep: \n",
    "    score = 1 - spatial.distance.cosine(idioms2embed_keep[idiom]['idiomatic'], idioms2embed_keep[idiom]['literal'])\n",
    "    idioms_embed_cossim.append(score)\n",
    "    idiom_to_cosim[idiom] = score\n",
    "print(\"mean inter-type cosine similarity (DiffSim):\")\n",
    "print(np.mean(idioms_embed_cossim))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e043985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_adapters]",
   "language": "python",
   "name": "conda-env-pytorch_adapters-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
